---
title: Development of Pre-trained transformer-based Models for Nepali Language
description: The 31st International Conference on Computational Linguistics (COLING 2025)  
slug: nepali-language-models
date: N/A
author: IRIIS
image: /first_image.png
url: https://arxiv.org/pdf/2411.15734
---


## Abstract

Transformer-based pre-trained language models have dominated the field of Natural Language Processing (NLP) for quite some time now. However, the Nepali language, spoken by approximately 32 million people worldwide, remains significantly underrepresented in this domain. This underrepresentation is primarily attributed to the scarcity of monolingual data corpora and limited available resources for the Nepali language. While existing efforts have predominantly concentrated on basic encoder-based models, there is a notable gap in the exploration of decoder-based architectures. To address this gap, we have collected 27.5 GB of Nepali text data, approximately 2.4x larger than any previously available Nepali language corpus. Leveraging this data, we pre-trained three different models i.e., BERT, RoBERTa, and GPT-2, exclusively for the Nepali Language. Furthermore, we performed instruction tuning and explored its potential for monolingual Nepali data, providing a foundation for future research. Our models outperformed the existing best model by 2 points on Nep-gLUE benchmark, scoring 95.60 and also outperformed existing models on text generation tasks, demonstrating improvements in both understanding and generating Nepali text.

### Citation Request

Please consider citing our work if you utilize any of our resources or results. Your acknowledgment would be greatly appreciated. Thank You!

```text
@misc{thapa2024,
      title={Development of Pre-Trained Transformer-based Models for the Nepali Language}, 
      author={Prajwal Thapa and Jinu Nyachhyon and Mridul Sharma and Bal Krishna Bal},
      year={2024},
      eprint={2411.15734},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.15734}, 
}
```

### Paper
Click below for the paper: